#################################################################################
######################### Microway Cluster Management Software (MCMS) for OpenHPC
#################################################################################
#
# Configuration for SLURM Resource Manager
#
#
# This file must be present on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
#################################################################################

ClusterName={clusterName}

ControlMachine={headName}
#ControlAddr=

# Specify a backup SLURM control server
# (note that both control servers must share a SLURM state filesystem)
#BackupController=
#BackupAddr=

SlurmdPort=6818
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdSpoolDir=/var/spool/slurmd
StateSaveLocation=/var/lib/slurmd

SlurmUser=slurm
#SlurmdUser=root



#################################################################################
# PROLOG, EPILOG AND HEALTH SCRIPTS

# Prepare for user jobs (must run very quickly)
#Prolog=

# Clean up after user jobs
Epilog=/etc/slurm/scripts/slurm.epilog.clean

#SrunProlog=
#SrunEpilog=

#TaskProlog=
#TaskEpilog=

# Prepare nodes for use (periodically run a longer health check)
PrologSlurmctld=/etc/slurm/scripts/slurmctld.prolog

#EpilogSlurmctld=

# Check health of all nodes in the cluster. This program must run very
# quickly, because it is automatically terminated after 60 seconds.
HealthCheckProgram=/etc/slurm/scripts/slurm.healthcheck

# Run health check every 15 minutes (900 seconds)
HealthCheckInterval=900



#################################################################################
AuthType=auth/munge
CryptoType=crypto/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=

CacheGroups=0
#GroupUpdateForce=0
#GroupUpdateTime=600

#DisableRootJobs=NO

# Rather than confuse users by accepting impossibly-sized jobs, such requests
# will be rejected when the user submits the job.
EnforcePartLimits=YES

#FirstJobId=1

# This is the most active jobs SLURM will support at a time
MaxJobCount=25000
#MaxJobId=999999

GresTypes=gpu,mic

#CheckpointType=checkpoint/none
#JobCheckpointDir=/var/slurm/checkpoint

#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=1
#KillOnBadExit=0
#LaunchType=launch/slurm

# If licenses need to be tracked on this cluster, list them here:
#Licenses=foo*4,bar

MailProg=/usr/bin/mailq

#MaxStepCount=40000
#MaxTasksPerNode=128

MpiDefault=none
#MpiParams=ports=12000-12999

#PluginDir=/usr/local/lib/slurm:/etc/slurm/plugins
PlugStackConfig=/etc/slurm/plugstack.conf

# Prevent users from seeing:
#  * reservations they cannot use
#  * other user's usage information
PrivateData=reservations,usage

ProctrackType=proctrack/cgroup
#PropagatePrioProcess=0
#PropagateResourceLimits=
PropagateResourceLimitsExcept=MEMLOCK

RebootProgram=/sbin/reboot

# Set this value to 2 if you want downed nodes to be returned to service,
# regardless of why they were set DOWN (e.g., unexpected reboots)
ReturnToService=1

#SallocDefaultCommand=

SwitchType=switch/none

TaskPlugin=task/cgroup
#TaskPluginParam=

#TopologyPlugin=topology/tree

# Specify which directory on compute nodes is considered temporary storage
#TmpFS=/tmp

#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0



#################################################################################
# TIMERS

#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
InactiveLimit=0
KillWait=30
MessageTimeout=30
MinJobAge=300
#ResvOverRun=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0

# Allow jobs to run 5 minutes longer than the time that was allocated to them
OverTimeLimit=5



#################################################################################
# SCHEDULING

# It's important to set a default in case users don't list their memory needs.
# On modern HPC clusters it's unusual to have less than 1GB per core
DefMemPerCPU=1024

FastSchedule=0
#MaxMemPerCPU=0
#SchedulerRootFilter=1
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerPort=7321

# Use 'select/linear' if you want to allocate whole nodes to each job
SelectType=select/cons_res

SelectTypeParameters=CR_Core_Memory

# By default, we set the following scheduling options:
#
#  * bf_interval sets how often SLURM works on backfilling jobs
#  * bf_continue is enabled to improve the ability of SLURM to backfill jobs
#  * bf_resolution is increased from 1 to 10 minutes, which will increase system
#    utilization (but may cause some jobs to start a few minutes late)
#  * bf_window is increased from 1 to 2 days, which causes SLURM to look further
#    into the future to determine when and where jobs can start
#  * bf_max_job_test sets the maximum number of jobs to try backfilling
#  * bf_max_job_part limits the number of jobs to backfill from one partition
#  * bf_max_job_user limits the number of backfilled jobs for any given user
#  * bf_max_job_start limits the number of backfill jobs to start at a time
#
SchedulerParameters=bf_interval=60,bf_continue,bf_resolution=600,bf_window=2880,bf_max_job_test=5000,bf_max_job_part=5000,bf_max_job_user=10,bf_max_job_start=100

# Allow higher-priority jobs to take resources from lower-priority jobs
PreemptType=preempt/partition_prio

# By default, a job which is preempted will simply be paused
PreemptMode=SUSPEND,GANG



#################################################################################
# JOB PRIORITY

PriorityFlags=FAIR_TREE,SMALL_RELATIVE_TO_TIME
PriorityType=priority/multifactor
PriorityDecayHalfLife=14-0
#PriorityCalcPeriod=
PriorityFavorSmall=NO

# All jobs more than a week old will be given the same age priority
PriorityMaxAge=7-0
#PriorityUsageResetPeriod=

# These values set the relative importance of each priority factor
PriorityWeightAge=1000
PriorityWeightFairshare=20000000
PriorityWeightJobSize=1000
PriorityWeightPartition=100000000
PriorityWeightQOS=1000000000



#################################################################################
# LOGGING AND ACCOUNTING

AccountingStorageEnforce=limits
AccountingStorageType=accounting_storage/slurmdbd
AccountingStoragePass=/var/run/munge/munge.socket.2

# If tracking GRES usage is desired, the names of the devices must be included:
AccountingStorageTRES=gres/Tesla_K40,gres/Tesla_K80,gres/Phi_7120P

AccountingStoreJobComment=YES

#DebugFlags=

# We do not worry about this plugin because SLURMDBD offers the same capability
JobCompType=jobcomp/none
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
#JobCompUser=

JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30

SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmctldDebug=3

SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdDebug=3

# By default, the scheduler does not write logs (can be enabled with scontrol)
SlurmSchedLogFile=/var/log/slurm/slurmsched.log
SlurmSchedLogLevel=0



#################################################################################
# POWER SAVE SUPPORT FOR IDLE NODES
SuspendProgram=/etc/slurm/scripts/slurmctld.power_nodes_off
ResumeProgram=/etc/slurm/scripts/slurmctld.power_nodes_on

# How long a node must be idle before it will be powered off (in seconds)
SuspendTime=14400   # Four hours

SuspendTimeout=30   # Number of seconds we expect the node shutdown to take
ResumeTimeout=300   # Number of seconds we expect the node boot process to take
ResumeRate=100      # Number of nodes we're willing to turn on at a time
SuspendRate=100     # Number of nodes we're willing to power off at a time

# Nodes and Partitions which should not be powered off
#SuspendExcNodes=
#SuspendExcParts=



#################################################################################
# COMPUTE NODES

NodeName=DEFAULT Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 State=UNKNOWN

# CPU-only compute nodes
NodeName=node[1-3]

# Compute Nodes with Accelerators/Coprocessors
#
# There's no need to set specific information about GPUs or Phis here - it will
# be automatically detected by each node during startup
#
#NodeName=node[4-32] Gres=gpu
#NodeName=node[33-64] Gres=mic

# Identify any nodes which are temporarily down
#
#DownNodes=node56 State=DOWN Reason=fan



#################################################################################
# PARTITION / QUEUE DEFINITIONS

# Set the number of nodes here - it will apply to the partitions below
PartitionName=DEFAULT Nodes=node[1-3]

# SLURM will use this value when users do not specify their job's length
PartitionName=DEFAULT DefaultTime=4:00:00

# By default we will not want SLURM to suspend jobs
PartitionName=DEFAULT PreemptMode=off

# If there are multiple login nodes, each must be listed here
#PartitionName=DEFAULT AllocNodes=login[1-2]


# Send most jobs here
#
# Take node that because SelectType is set to 'select/cons_res', this partition
# will schedule multiple jobs an each compute node. However, it will not force
# jobs to share CPU cores - they'll each receive their own dedicated CPU cores.
#
PartitionName=normal Nodes=ALL Priority=10 Default=YES MaxTime=7-0:00:00 Shared=FORCE:1


# Users debugging their runs should submit here
PartitionName=debug Nodes=ALL Priority=12 DefaultTime=30:00 MaxTime=30:00 MaxNodes=4


# Interactive/Realtime sessions are higher priority than queued batch jobs.
PartitionName=interactive Priority=14 Nodes=node[1-3] DefaultTime=4:00:00 MaxTime=8:00:00 MaxNodes=4 MaxCPUsPerNode=2 MaxMemPerNode=4096


# Administrators/Operators can test and debug the cluster here - regular users
# will not be able to submit jobs to this partition
PartitionName=admin Nodes=ALL Priority=14 DefaultTime=30:00 MaxTime=30:00 AllowGroups=hpc-admin


# Pre-emptable Partitions
#
# SLURM allows users to submit jobs that are later paused, interrupted or
# rescheduled. This helps give users immediate access to resources they might
# not otherwise have access to (with the understanding that those resources may
# only be available to them for a short period of time). Although other options
# are available with SLURM, the following preemption QOS options are used below:
#
#  * Requeue: Requeue the job (it will be killed and then started again later)
#  * Suspend: Suspend the lower priority job and automatically resume it when
#    the higher priority job terminates (re-uses some gang scheduling logic).
#    For this to work, memory use must be managed/monitored by SLURM.
#
# These partitions have very lax restrictions, but do not guarantee that the job
# will have uninterrupted access to the resources. The GraceTime parameter
# allows preempted jobs to clean themselves up before being cancelled (note that
# the application must cleanly handle SIGCONT and SIGTERM for this to work).
#
PartitionName=DEFAULT Priority=2 MaxTime=14-0:00:00 Shared=FORCE:1 GraceTime=30

# Jobs which are designed cleanly (which means handling SIGCONT and SIGTERM)
# should be submitted to 'reschedulable'. When preempted, they will be
# cancelled (freeing all resources) and rescheduled for later execution. We want
# to incentivize users to build this type of job, as it is the cleanest method.
PartitionName=reschedulable Nodes=ALL PreemptMode=REQUEUE

# Jobs which cannot properly be cancelled and rescheduled should be submitted
# to 'pausable'. When preempted, they will be paused/suspended (but will remain
# in memory). Once the high-priority jobs are finished, these jobs will be
# resumed. This is fairly clean, but may cause contention for memory.
PartitionName=pausable Nodes=ALL PreemptMode=SUSPEND

