#################################################################################
######################### Microway Cluster Management Software (MCMS) for OpenHPC
#################################################################################
#
# Configuration for SLURM Resource Manager
#
#
# This file must be present on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
#################################################################################

ClusterName={clusterName}

ControlMachine={headName}
#ControlAddr=
#BackupController=
#BackupAddr=

SlurmdPort=6818
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd/slurmd.pid
SlurmctldPidFile=/var/run/slurmd/slurmctld.pid
SlurmdSpoolDir=/var/spool/slurmd
StateSaveLocation=/var/lib/slurmd
SlurmUser=slurm
#SlurmdUser=root


# PROLOG, EPILOG AND HEALTH SCRIPTS

# Prepare for user jobs (must run very quickly)
#Prolog=
#
# Clean up after user jobs
Epilog=/etc/slurm/scripts/slurm.epilog.clean

#SrunProlog=
#SrunEpilog=

#TaskProlog=
#TaskEpilog=

# Prepare nodes for use (periodically run a longer health check)
PrologSlurmctld=/etc/slurm/scripts/slurmctld.prolog
#
#EpilogSlurmctld=

# Check health of all nodes in the cluster. This program must run very
# quickly, because it is automatically terminated after 60 seconds.
HealthCheckProgram=/etc/slurm/scripts/slurm.healthcheck
#
# Run health check every 15 minutes (900 seconds)
HealthCheckInterval=900


AuthType=auth/munge
CacheGroups=0
#GroupUpdateForce=0
#GroupUpdateTime=600
CryptoType=crypto/munge
#DisableRootJobs=NO
#EnforcePartLimits=NO
#FirstJobId=1
MaxJobCount=25000
#MaxJobId=999999
GresTypes=gpu,mic
#CheckpointType=checkpoint/none
#JobCheckpointDir=/var/slurm/checkpoint
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=1
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=foo*4,bar
#MailProg=/bin/mail
#MaxStepCount=40000
#MaxTasksPerNode=128
MpiDefault=none
#MpiParams=ports=12000-12999
#PluginDir=
PlugStackConfig=/etc/slurm/plugstack.conf
#PrivateData=jobs
ProctrackType=proctrack/cgroup
#PropagatePrioProcess=0
#PropagateResourceLimits=
PropagateResourceLimitsExcept=MEMLOCK
RebootProgram=/sbin/reboot
ReturnToService=1
#SallocDefaultCommand=
SwitchType=switch/none
TaskPlugin=task/cgroup
#TaskPluginParam=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0


# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
InactiveLimit=0
KillWait=30
MessageTimeout=30
MinJobAge=300
OverTimeLimit=10
#ResvOverRun=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0


# SCHEDULING
#DefMemPerCPU=0
FastSchedule=0
#MaxMemPerCPU=0
#SchedulerRootFilter=1
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerPort=7321
# Use a SelectType of select/linear if you want to allocate whole nodes to each job
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory


# JOB PRIORITY
#PriorityFlags=
PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=1-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightAge=1000
#PriorityWeightFairshare=100000
#PriorityWeightJobSize=
#PriorityWeightPartition=10000
#PriorityWeightQOS=


# LOGGING AND ACCOUNTING
AccountingStorageEnforce=limits
#AccountingStorageHost=
#AccountingStorageLoc=
AccountingStoragePass=/var/run/munge/munge.socket.2
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=
AccountingStoreJobComment=YES
#DebugFlags=
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
#SlurmSchedLogFile=
#SlurmSchedLogLevel=


# POWER SAVE SUPPORT FOR IDLE NODES
SuspendProgram=/etc/slurm/scripts/slurmctld.power_nodes_off
ResumeProgram=/etc/slurm/scripts/slurmctld.power_nodes_on
# How long a node must be idle before it will be powered off (in seconds)
SuspendTime=14400   # Four hours
SuspendTimeout=30   # Number of seconds we expect the node shutdown to take
ResumeTimeout=300   # Number of seconds we expect the node boot process to take
ResumeRate=100      # Number of nodes we're willing to turn on at a time
SuspendRate=100     # Number of nodes we're willing to power off at a time
#SuspendExcNodes=
#SuspendExcParts=


# COMPUTE NODES
NodeName=DEFAULT Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 State=UNKNOWN
NodeName=node[1-32] Gres=gpu
NodeName=node[33-64] Gres=mic


# PARTITION / QUEUE DEFINITIONS

# Long jobs must be sent here (shorter jobs can be assigned higher priorities below)
PartitionName=month-long Priority=5000 Nodes=node[1-64] Default=NO MaxTime=31-0:00:00 State=UP MaxNodes=8

# Normal jobs will be sent here
PartitionName=normal Priority=20000 Nodes=node[1-64] Default=YES MaxTime=1-0:00:00 State=UP


# Interactive sessions are considered higher priority than batch jobs.

# Specify part of a node per interactive session:
PartitionName=interactive Priority=50000 Nodes=node[1-15] Shared=YES:8 DefaultTime=4:00:00 MaxTime=8:00:00 State=UP MaxNodes=1 MaxCPUsPerNode=2 MaxMemPerNode=10000


